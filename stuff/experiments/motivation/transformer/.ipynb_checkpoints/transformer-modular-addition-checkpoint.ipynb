{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3479933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange, repeat\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DecoderBlock(torch.nn.Module):\n",
    "  def __init__(self, dim_model=128, n_heads=4):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attn = nn.MultiheadAttention(dim_model, n_heads)  # According to the paper.\n",
    "    \n",
    "    # In practice hidden layer size is 4 times size of input.\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(dim_model, dim_model * 4),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_model * 4, dim_model)\n",
    "    ) \n",
    "    \n",
    "  def forward(self, x):\n",
    "    attn_mask = torch.full(\n",
    "        (len(x), len(x)), -float(\"Inf\"), device=x.device, dtype=x.dtype\n",
    "    )\n",
    "    attn_mask = torch.triu(attn_mask, diagonal=1)  # Autoregressive model, hence only look back.\n",
    "    \n",
    "    a1, _ = self.self_attn(x, x, x, attn_mask=attn_mask)  # We do not need attention weights.\n",
    "    a2 = self.mlp(x + a1)  # Add skip connection / residual connection.\n",
    "    return a2\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "  def __init__(self, num_layers=1, dim_model=128, num_heads=4, num_tokens=114, seq_len=3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.token_embeddings = nn.Embedding(num_tokens, dim_model)  # We have p+1 input tokens: 0,1,...,113.\n",
    "    self.position_embeddings = nn.Embedding(seq_len, dim_model)  # We length 3 sequences, e.g. (10, 25, 113)\n",
    "    self.model = nn.Sequential(\n",
    "        *[DecoderBlock(dim_model, num_heads) for _ in range(num_layers)],\n",
    "        nn.Linear(dim_model, num_tokens - 1)  # We have p output tokens for the modulo operation: 0,1,...,112.\n",
    "    ) \n",
    "\n",
    "  def forward(self, inputs):\n",
    "    token_embedding = self.token_embeddings(inputs)\n",
    "    \n",
    "    positions = repeat(torch.arange(inputs.shape[1]), \"p -> b p\", b = inputs.shape[0])\n",
    "    position_embedding = self.position_embeddings(positions)\n",
    "    \n",
    "    embedding = token_embedding + position_embedding\n",
    "\n",
    "    embedding = rearrange(embedding, 'b s d -> s b d')\n",
    "\n",
    "    return self.model(embedding)[-1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753a8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data(p: int, threshold: float):  \n",
    "#     dataset = []\n",
    "#     for i in range(p):\n",
    "#         for j in range(p):\n",
    "#             dataset.append([i, j, p])\n",
    "#     dataset = torch.Tensor(dataset).to(torch.int64)\n",
    "    \n",
    "#     labels = (dataset[:, 0] + dataset[:, 1]) % p\n",
    "    \n",
    "#     idxs = torch.randperm(p ** 2)\n",
    "#     cutoff = int(threshold * p ** 2)\n",
    "#     train_indices = idxs[:cutoff]\n",
    "#     test_indices = idxs[cutoff:]\n",
    "    \n",
    "    \n",
    "#     train_data = dataset[train_indices]\n",
    "#     train_labels = labels[train_indices]\n",
    "    \n",
    "#     test_data = dataset[test_indices]\n",
    "#     test_labels = labels[test_indices]\n",
    "    \n",
    "#     return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "#path = '/Users/.../Desktop/gdl/motivation/'\n",
    "\n",
    "train_data = torch.load(path + 'train_data.pt')\n",
    "train_labels = torch.load(path + 'train_labels.pt')\n",
    "test_data = torch.load(path + 'test_data.pt')\n",
    "test_labels = torch.load(path + 'test_labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f7d24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, train_labels, test_data, test_labels = get_data(113, 0.3)\n",
    "model = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0e36a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3830, 3])\n",
      "torch.Size([3830])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9fd688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"b_\" in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29eb9db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 113\n",
    "\n",
    "# Optimizer config\n",
    "lr = 1e-3\n",
    "wd = 1. \n",
    "betas = (0.9, 0.98)\n",
    "\n",
    "num_epochs = 50000\n",
    "checkpoint_every = 100\n",
    "\n",
    "DATA_SEED = 598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "270a8064",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae6b8713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7484, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor(4.7460, dtype=torch.float64, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    logits = logits.to(torch.float64)\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
    "    return -correct_log_probs.mean()\n",
    "train_logits = model(train_data)\n",
    "train_loss = loss_fn(train_logits, train_labels)\n",
    "print(train_loss)\n",
    "test_logits = model(test_data)\n",
    "test_loss = loss_fn(test_logits, test_labels)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45bf61d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform loss:\n",
      "4.727387818712341\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Uniform loss:\")\n",
    "print(np.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51200e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [3:10:59<00:00,  4.36it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm.auto as tqdm\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    train_logits = model(train_data)\n",
    "    train_loss = loss_fn(train_logits, train_labels)\n",
    "    train_loss.backward()\n",
    "    train_losses.append(train_loss.item())\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model(test_data)\n",
    "        test_loss = loss_fn(test_logits, test_labels)\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577956e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

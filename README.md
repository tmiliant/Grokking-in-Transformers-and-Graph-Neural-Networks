# Grokking-in-Transformers-and-Graph-Neural-Networks

Transformers are essentially equivalent (except for the attention mechanism, since gnn-s do not have query, key, value) to graph attention networks. I compare the two on simple algorithmic tasks, and show that despite the equivalence, we see grokking for transformers, but not for their equivalent graph attention networks.

# Grokking-in-Transformers-and-Graph-Neural-Networks

Transformers are essentially equivalent to graph attention networks (except for the attention mechanism, since gnn-s do not have query, key, value). I compare the two on simple algorithmic tasks, and show that despite the equivalence, we see grokking for transformers, but not for their equivalent graph attention networks.

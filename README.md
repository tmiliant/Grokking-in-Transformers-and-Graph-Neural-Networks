# Grokking-in-Transformers-and-Graph-Neural-Networks

Transformers are essentially equivalent (except for the attention mechanism) to graph attention networks. I compare them on simple algorithmic tasks, and show that despite the equivalence, we see grokking for transformers, but not for their equivalent graph attention networks.
